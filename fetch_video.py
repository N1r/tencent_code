import asyncio
import aiohttp
import pandas as pd
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
from tqdm.asyncio import tqdm
import os
import platform
import random
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

CONFIG = {
    'API_KEY': 'AIzaSyDMVvDeq4xHWFpTh5hGRiZoBettBqrSbcs',
    #'API_KEY': 'AIzaSyABmSbMC15Uf0xVn6NWzNpUG9b9l3a5yaY', 
    'CHANNELS': {
        # === é¡¶çº§é«˜é¢‘æºï¼ˆæ—¥æ›´10-20æ¡ï¼‰===
        'MeidasTouch': 'UC9r9HYFxEQOBXSopFS61ZWg',              # 570ä¸‡è®¢é˜…ï¼Œåç‰¹æœ—æ™®
        #'The Hill': 'UCPWXiRWZ29zrxPFIQT7eHSA',                # 270ä¸‡è®¢é˜…ï¼Œç‹¬ç«‹æ–°é—»
       # 'Forbes Breaking News': 'UCuTiq7iBWzbKfvTqNhUz7bg',    # å›½ä¼šå¬è¯å‰ªè¾‘
        #'Tucker Carlson': 'UCxwubvG70lardn6CkfVdnSw',          # 175ä¸‡è®¢é˜…ï¼Œä¿å®ˆæ´¾
        
        # === ä¸»æµåª’ä½“ï¼ˆæ—¥æ›´5-10æ¡ï¼‰===
        #'NBC News': 'UCeY0bbntWzzVIaj2z3QigXg',                # 1140ä¸‡è®¢é˜…
        'CNN': 'UCupvZG-5ko_eiXAupbDfxWw',                      # 1890ä¸‡è®¢é˜…
        'Fox News': 'UCXIJgqnII2ZOINSWNOGFThA',                # 1240ä¸‡è®¢é˜…
        #'ABC News': 'UCBi2mrWuNuyYy4gbM6fU18Q',                # 1780ä¸‡è®¢é˜…
        
        # === å›½ä¼š/ä¸“é¢˜===
        'The Hill': 'UCPWXiRWZ29zrxPFIQT7eHSA',
        'Forbes Breaking News': 'UCg40OxZ1GYh3u3jBntB6DLg',  # åŸID: UCg40OxZ1GYh3u3jBntB6DLg
        'Congress Clips': 'UUJQFbOJfbN6ZjJ3R5AvxNyg',  # åŸID: UCJQFbOJfbN6ZjJ3R5AvxNyg
        'The Stephen A. Smith Show': 'UU2OREBiIbDChxvmDeg30Bsg',  # åŸID: UC2OREBiIbDChxvmDeg30Bsg
        'Benny Johnson': 'UULdP3jmBYe9lAZQbY6OSYjw',  # åŸID: UCLdP3jmBYe9lAZQbY6OSYjw
 
        # === ä¸ªäººä¸»æ’­ï¼ˆæ—¥æ›´3-8æ¡ï¼‰===
        'BTC': 'UCQANb2YPwAtK-IQJrLaaUFw',                      # Brian Tyler Cohenï¼Œå·¦ç¿¼
        #'Ben Shapiro': 'UCnQC_G5Xsjhp9fEJKuIcrSw',             # 650ä¸‡è®¢é˜…ï¼Œå³ç¿¼
        'Benny Johnson': 'UCfiCnGMHYrEWU97NAdzQ1Fw',           # ä¿å®ˆæ´¾Meme
        'The David Pakman Show': 'UCvixJtaXuNdMPUGdOPcY8Ag',   # 220ä¸‡è®¢é˜…ï¼Œè¿›æ­¥æ´¾
    },
    
    'MAX_RESULTS_PER_CHANNEL': 10,
    
    'VIDEO_FILTERS': {
        'MIN_DURATION': 150,      # 2åˆ†é’Ÿ
        'MAX_DURATION': 500,      # 10åˆ†é’Ÿ
        'MIN_VIEWS': 1000,
        'MIN_COMMENTS': 10
    },
    
    'SELECTION': {
        'NUM_CHANNELS': 10,                    # éšæœºé€‰æ‹©çš„é¢‘é“æ•°
        'VIDEOS_PER_CHANNEL_MIN': 1,          # æ¯ä¸ªé¢‘é“æœ€å°‘è§†é¢‘æ•°
        'VIDEOS_PER_CHANNEL_MAX': 2,          # æ¯ä¸ªé¢‘é“æœ€å¤šè§†é¢‘æ•°
        'TOP_N_CANDIDATES': 5                # ä»æ¯ä¸ªé¢‘é“è¯„è®ºæ•°å‰Nåä¸­é€‰æ‹©
    }
}
@dataclass
class YouTubeConfig:
    API_KEY: str = CONFIG['API_KEY']
    BASE_URL: str = "https://www.googleapis.com/youtube/v3"
    MAX_RESULTS: int = CONFIG['MAX_RESULTS_PER_CHANNEL']
    VIDEO_FILTERS: dict = field(default_factory=lambda: CONFIG['VIDEO_FILTERS'])
    MAX_RETRIES: int = 3
    RETRY_DELAY: float = 2.0

class YouTubeAPI:
    def __init__(self, config: YouTubeConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            await asyncio.sleep(0.25)  # ç¡®ä¿è¿æ¥å®Œå…¨å…³é—­
            self.session = None

    async def _make_request(self, url: str, params: dict = None) -> Optional[dict]:
        """å¸¦é‡è¯•æœºåˆ¶çš„HTTPè¯·æ±‚"""
        for attempt in range(self.config.MAX_RETRIES):
            try:
                async with self.session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if 'error' in data:
                            logger.error(f"API Error: {data['error']['message']}")
                            return None
                        return data
                    elif response.status == 403:
                        logger.error("API quota exceeded or invalid API key")
                        return None
                    else:
                        logger.warning(f"HTTP {response.status}, retrying... ({attempt + 1}/{self.config.MAX_RETRIES})")
            except asyncio.TimeoutError:
                logger.warning(f"Timeout on attempt {attempt + 1}/{self.config.MAX_RETRIES}")
            except Exception as e:
                logger.error(f"Request failed: {e}")
            
            if attempt < self.config.MAX_RETRIES - 1:
                await asyncio.sleep(self.config.RETRY_DELAY * (attempt + 1))
        
        return None

    async def get_latest_videos(self, channel_id: str, channel_name: str, max_results: int = 10) -> List[Dict[Any, Any]]:
        """è·å–é¢‘é“æœ€æ–°è§†é¢‘"""
        logger.info(f"Fetching videos from: {channel_name}")
        
        # ç¬¬ä¸€æ­¥ï¼šæœç´¢æœ€æ–°è§†é¢‘
        search_url = f"{self.config.BASE_URL}/search"
        search_params = {
            "part": "snippet",
            "channelId": channel_id,
            "order": "date",
            "maxResults": max_results,
            "type": "video",
            "key": self.config.API_KEY
        }
        
        search_data = await self._make_request(search_url, search_params)
        if not search_data or 'items' not in search_data:
            logger.warning(f"No videos found for {channel_name}")
            return []

        video_ids = [
            item['id']['videoId']
            for item in search_data['items']
            if item['id']['kind'] == 'youtube#video'
        ]

        if not video_ids:
            return []

        # ç¬¬äºŒæ­¥ï¼šè·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
        videos_url = f"{self.config.BASE_URL}/videos"
        videos_params = {
            "part": "snippet,contentDetails,statistics",
            "id": ",".join(video_ids),
            "key": self.config.API_KEY
        }

        videos_data = await self._make_request(videos_url, videos_params)
        if not videos_data or 'items' not in videos_data:
            return []

        videos = []
        for item in videos_data['items']:
            video = self._parse_video_data(item, channel_name)
            if video:
                videos.append(video)
        
        logger.info(f"Found {len(videos)} valid videos from {channel_name}")
        return videos

    def _parse_video_data(self, item: Dict, channel_name: str) -> Optional[Dict]:
        """è§£æè§†é¢‘æ•°æ®"""
        try:
            duration = self._parse_duration(item['contentDetails'].get('duration', 'PT0S'))
            view_count = int(item['statistics'].get('viewCount', 0))
            comment_count = int(item['statistics'].get('commentCount', 0))

            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶
            if not self._meets_criteria(duration, view_count, comment_count):
                return None

            return {
                'videoId': item['id'],
                'title': item['snippet']['title'],
                'description': item['snippet']['description'][:200],  # é™åˆ¶æè¿°é•¿åº¦
                'publishedAt': item['snippet']['publishedAt'],
                'duration': duration,
                'viewCount': view_count,
                'commentCount': comment_count,
                'channel_name': channel_name,
            }
        except Exception as e:
            logger.error(f"Error parsing video data: {e}")
            return None

    def _meets_criteria(self, duration: int, views: int, comments: int) -> bool:
        """æ£€æŸ¥è§†é¢‘æ˜¯å¦ç¬¦åˆç­›é€‰æ¡ä»¶"""
        filters = self.config.VIDEO_FILTERS
        return (
            filters['MIN_DURATION'] <= duration <= filters['MAX_DURATION'] and
            views >= filters['MIN_VIEWS'] and
            comments >= filters['MIN_COMMENTS']
        )

    @staticmethod
    def _parse_duration(duration_str: str) -> int:
        """è§£æISO 8601æ—¶é•¿æ ¼å¼"""
        hours = minutes = seconds = 0
        duration_str = duration_str.replace('PT', '')
        
        if 'H' in duration_str:
            parts = duration_str.split('H')
            hours = int(parts[0])
            duration_str = parts[1]
        
        if 'M' in duration_str:
            parts = duration_str.split('M')
            minutes = int(parts[0])
            duration_str = parts[1]
        
        if 'S' in duration_str:
            seconds = int(duration_str.replace('S', ''))
        
        return hours * 3600 + minutes * 60 + seconds

class YouTubeDataProcessor:
    def __init__(self, videos_data: List[Dict]):
        self.videos_data = [v for v in videos_data if v is not None]

    def process_data(self) -> pd.DataFrame:
        """å¤„ç†è§†é¢‘æ•°æ®å¹¶ç”ŸæˆExcel"""
        if not self.videos_data:
            logger.warning("No videos to process")
            return pd.DataFrame()

        df = pd.DataFrame(self.videos_data)
        logger.info(f"Total videos before filtering: {len(df)}")
        
        # æ·»åŠ å¿…éœ€åˆ—
        df['Video File'] = 'https://www.youtube.com/watch?v=' + df['videoId']
        df['Source Language'] = 'en'
        df['Target Language'] = 'ç®€ä½“ä¸­æ–‡'
        df['Dubbing'] = 0
        df['Status'] = ''

        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        df_full = df.copy()
        df_full.to_excel('batch/all_videos.xlsx', index=False)
        logger.info(f"Saved full dataset: batch/all_videos.xlsx ({len(df_full)} videos)")

        # æ™ºèƒ½é€‰æ‹©è§†é¢‘
        df_selected = self._smart_selection(df)
        
        # åˆå¹¶ç°æœ‰æ•°æ®
        df_final = self._merge_with_existing(df_selected)
        
        return df_final[['Video File', 'title', 'description', 'viewCount', 
                        'channel_name', 'duration', 'Source Language', 
                        'Target Language', 'Dubbing', 'Status']]

    def _smart_selection(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ™ºèƒ½é€‰æ‹©è§†é¢‘"""
        config = CONFIG['SELECTION']
        
        # éšæœºé€‰æ‹©é¢‘é“
        unique_channels = df['channel_name'].unique()
        num_channels = min(config['NUM_CHANNELS'], len(unique_channels))
        selected_channels = random.sample(list(unique_channels), num_channels)
        
        logger.info(f"Selected {num_channels} channels: {', '.join(selected_channels)}")
        
        # ä»æ¯ä¸ªé¢‘é“é€‰æ‹©è§†é¢‘
        df_filtered = df[df['channel_name'].isin(selected_channels)].copy()
        
        # æŒ‰é¢‘é“å’Œè¯„è®ºæ•°æ’åº
        df_filtered = df_filtered.sort_values(
            by=['channel_name', 'commentCount'], 
            ascending=[True, False]
        )
        
        # ä»æ¯ä¸ªé¢‘é“çš„å‰Nåä¸­éšæœºé€‰æ‹©1-2ä¸ª
        selected_videos = []
        for channel in selected_channels:
            channel_videos = df_filtered[df_filtered['channel_name'] == channel]
            
            # è·å–å‰Nä¸ªå€™é€‰è§†é¢‘
            candidates = channel_videos.head(config['TOP_N_CANDIDATES'])
            
            # éšæœºé€‰æ‹©1-2ä¸ª
            n_select = min(
                random.randint(config['VIDEOS_PER_CHANNEL_MIN'], config['VIDEOS_PER_CHANNEL_MAX']),
                len(candidates)
            )
            
            selected = candidates.sample(n=n_select)
            selected_videos.append(selected)
            
            logger.info(f"  {channel}: selected {n_select} from {len(candidates)} candidates")
        
        # åˆå¹¶å¹¶æ‰“ä¹±é¡ºåº
        df_result = pd.concat(selected_videos, ignore_index=True)
        df_result = df_result.sample(frac=1).reset_index(drop=True)
        
        # ä¿å­˜æœ¬æ¬¡æ–°é€‰æ‹©çš„è§†é¢‘
        df_result.to_excel('batch/new_videos.xlsx', index=False)
        logger.info(f"Saved newly selected videos: batch/new_videos.xlsx ({len(df_result)} videos)")
        
        return df_result

    def _merge_with_existing(self, df_new: pd.DataFrame) -> pd.DataFrame:
        """ä¸ç°æœ‰æ•°æ®åˆå¹¶"""
        try:
            existing_df = pd.read_excel('batch/tasks_setting.xlsx')
            logger.info(f"Found existing tasks file with {len(existing_df)} entries")
            
            # åˆå¹¶æ•°æ®
            df_combined = pd.concat([existing_df, df_new], ignore_index=True)
            
            # å»é‡
            df_combined = df_combined.drop_duplicates(subset=['Video File'], keep='first')
            
            logger.info(f"After merging and deduplication: {len(df_combined)} videos")
            return df_combined
            
        except FileNotFoundError:
            logger.info("No existing tasks file found, creating new one")
            return df_new
        except Exception as e:
            logger.error(f"Error merging with existing file: {e}")
            return df_new

async def main():
    """ä¸»å‡½æ•°"""
    config = YouTubeConfig()
    channels = CONFIG['CHANNELS']

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('batch', exist_ok=True)

    logger.info(f"Starting to fetch data for {len(channels)} channels...")
    logger.info(f"Video filters: {CONFIG['VIDEO_FILTERS']}")

    async with YouTubeAPI(config) as api:
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [
            api.get_latest_videos(channel_id, channel_name, CONFIG['MAX_RESULTS_PER_CHANNEL'])
            for channel_name, channel_id in channels.items()
        ]
        
        # ä½¿ç”¨è¿›åº¦æ¡æ‰§è¡Œä»»åŠ¡
        all_videos_data = []
        with tqdm(total=len(tasks), desc="Fetching channels") as pbar:
            for coro in asyncio.as_completed(tasks):
                videos = await coro
                all_videos_data.append(videos)
                pbar.update(1)

        # å±•å¹³è§†é¢‘åˆ—è¡¨
        all_videos = [video for videos in all_videos_data for video in videos]
        logger.info(f"Total videos fetched: {len(all_videos)}")

        # å¤„ç†æ•°æ®
        logger.info("Processing video data...")
        processor = YouTubeDataProcessor(all_videos)
        df = processor.process_data()

        # ä¿å­˜ç»“æœ
        output_path = 'batch/tasks_setting.xlsx'
        df.to_excel(output_path, index=False)
        
        logger.info(f"\n{'='*50}")
        logger.info(f"âœ… Successfully processed {len(df)} videos")
        logger.info(f"ğŸ“ Results saved to: {output_path}")
        logger.info(f"{'='*50}\n")

if __name__ == "__main__":
    # Windowså…¼å®¹æ€§è®¾ç½®
    if platform.system() == 'Windows':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    # è¿è¡Œä¸»ç¨‹åº
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Program interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}", exc_info=True)
