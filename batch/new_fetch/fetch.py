import requests
import pandas as pd
import os
import logging
from datetime import datetime
from tqdm import tqdm

# ============= 1. é…ç½®åŒºåŸŸ =============

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

CONFIG = {
    # ç›®æ ‡è´¦å·åˆ—è¡¨
    'TARGETS': [
        "acyn.bsky.social",
        "atrupar.bsky.social",
        "ronfilipkowski.bsky.social",
        "patriottakes.bsky.social",
        "meidastouch.bsky.social",
        "kamalahq.bsky.social",
        "waltermasterson.bsky.social",
        "thegoodliars.bsky.social",
    ],

    # æ¯ä¸ªè´¦å·æ£€æŸ¥æœ€è¿‘å¤šå°‘æ¡ï¼Ÿ
    'CHECK_LIMIT': 50,
    
    # ç»“æœä¿å­˜æ–‡ä»¶å
    'OUTPUT_FILE': 'bsky_tasks.csv'
}

# ============= 2. Bluesky å·¥å…·ç±» =============

class BlueskyFetcher:
    def __init__(self):
        self.api_root = "https://public.api.bsky.app/xrpc"

    def resolve_handle(self, handle):
        """å°†ç”¨æˆ·åè½¬ä¸º DID"""
        try:
            url = f"{self.api_root}/com.atproto.identity.resolveHandle"
            res = requests.get(url, params={"handle": handle}, timeout=10)
            if res.status_code == 200:
                return res.json().get("did")
        except Exception as e:
            logger.error(f"è§£æç”¨æˆ· {handle} å¤±è´¥: {e}")
        return None

    def get_latest_videos(self, handle, limit=50):
        """è·å–ç”¨æˆ·çš„æ—¶é—´çº¿è§†é¢‘"""
        did = self.resolve_handle(handle)
        if not did:
            return []

        url = f"{self.api_root}/app.bsky.feed.getAuthorFeed"
        params = {
            "actor": did,
            "limit": limit,
            "filter": "posts_with_video" 
        }

        try:
            res = requests.get(url, params=params, timeout=15)
            data = res.json()
            if "feed" not in data:
                return []
            
            videos = []
            for item in data["feed"]:
                post = item.get("post", {})
                record = post.get("record", {})
                
                uri = post.get("uri", "")
                if not uri: continue
                post_id = uri.split("/")[-1]
                post_url = f"https://bsky.app/profile/{handle}/post/{post_id}"
                
                # æå–å®Œæ•´æ–‡å­—
                raw_text = record.get("text", "")
                
                # æå–ç®€çŸ­æ ‡é¢˜ (å»æ¢è¡Œ)
                clean_title = raw_text.replace("\n", " ").strip()[:50]
                if not clean_title:
                    clean_title = f"Video_{post_id}"
                
                created_at = record.get("createdAt", "")
                
                videos.append({
                    "Handle": handle,
                    "Date": created_at[:10],
                    "Title": clean_title,
                    "Full Text": raw_text, # ä¿ç•™å®Œæ•´æ–‡å­—
                    "Post URL": post_url,  # å»é‡å”¯ä¸€æ ‡è¯†
                    "Post ID": post_id
                })
            
            return videos
            
        except Exception as e:
            logger.error(f"æŠ“å– {handle} å¤±è´¥: {e}")
            return []

# ============= 3. æ ¸å¿ƒï¼šåˆå¹¶ä¸å»é‡é€»è¾‘ =============

def merge_and_save(new_data_list):
    """
    å°†æ–°æŠ“å–çš„æ•°æ®ä¸ CSV ä¸­çš„æ—§æ•°æ®åˆå¹¶å¹¶å»é‡
    """
    filename = CONFIG['OUTPUT_FILE']
    
    # 1. å°†æ–°æ•°æ®è½¬ä¸º DataFrame
    new_df = pd.DataFrame(new_data_list)
    
    if new_df.empty:
        print("âš ï¸ æœ¬æ¬¡æœªæŠ“å–åˆ°ä»»ä½•æ•°æ®ã€‚")
        return

    # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§æ–‡ä»¶
    if os.path.exists(filename):
        try:
            old_df = pd.read_csv(filename)
            original_count = len(old_df)
            
            # åˆå¹¶æ—§æ•°æ®å’Œæ–°æ•°æ®
            combined_df = pd.concat([old_df, new_df], ignore_index=True)
            
            # 3. å»é‡ (æ ¸å¿ƒæ­¥éª¤)
            # subset=['Post URL']: æ ¹æ®é“¾æ¥åˆ¤æ–­æ˜¯å¦é‡å¤
            # keep='first': ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„(æ—§çš„)ï¼Œè¿™æ ·ä¸ä¼šæ‰“ä¹±åŸæœ‰é¡ºåºï¼Œä¹Ÿå¯ä»¥é€‰ 'last'
            final_df = combined_df.drop_duplicates(subset=['Post URL'], keep='first')
            
            new_added_count = len(final_df) - original_count
            
            if new_added_count > 0:
                print(f"ğŸ”„ åˆå¹¶å®Œæˆï¼šåº“ä¸­åŸæœ‰ {original_count} æ¡ï¼Œæœ¬æ¬¡æ–°å¢ {new_added_count} æ¡ã€‚")
            else:
                print(f"ğŸ’¤ æ²¡æœ‰å‘ç°æ–°è§†é¢‘ (åº“ä¸­å·²æœ‰ {original_count} æ¡)ã€‚")
                
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ—§æ–‡ä»¶å‡ºé”™ ({e})ï¼Œå°†ç›´æ¥è¦†ç›–ä¿å­˜ã€‚")
            final_df = new_df
            new_added_count = len(final_df)
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥ä¿å­˜
        final_df = new_df
        new_added_count = len(final_df)
        print(f"ğŸ†• åˆ›å»ºæ–°æ–‡ä»¶ï¼Œå…± {new_added_count} æ¡ã€‚")

    # 4. æ’åº (æŒ‰æ—¥æœŸé™åºï¼Œè®©æœ€æ–°çš„åœ¨æœ€ä¸Šé¢)
    if 'Date' in final_df.columns:
        final_df = final_df.sort_values(by="Date", ascending=False)

    # 5. ä¿å­˜
    final_df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜è‡³: {filename}")

# ============= 4. ä¸»ç¨‹åº =============

def main():
    fetcher = BlueskyFetcher()
    current_batch_videos = []
    
    print(f"ğŸš€ å¼€å§‹æ‰«æ Bluesky, ç›®æ ‡è´¦å·: {len(CONFIG['TARGETS'])} ä¸ª")
    
    with tqdm(total=len(CONFIG['TARGETS'])) as pbar:
        for user in CONFIG['TARGETS']:
            pbar.set_description(f"æ‰«æ {user}")
            videos = fetcher.get_latest_videos(user, limit=CONFIG['CHECK_LIMIT'])
            current_batch_videos.extend(videos)
            pbar.update(1)

    print("\n" + "-"*50)
    # è°ƒç”¨åˆå¹¶å»é‡å‡½æ•°
    merge_and_save(current_batch_videos)
    print("-"*50)

if __name__ == "__main__":
    main()